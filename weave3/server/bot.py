#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""weave3 - Aetheris V.O.

This bot uses a realtime pipeline: Speech-to-Speech with integrated LLM

Generated by Pipecat CLI

Required AI services:
- Ultravox (Realtime Speech-to-Speech)

Run the bot using::
    uv run bot.py
"""

import asyncio
import datetime
import os
import time
import uuid

import requests
from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    ErrorFrame,
    Frame,
    InputTextRawFrame,
    LLMRunFrame,
    TranscriptionFrame,
    UserImageRawFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import (
    LLMContextAggregatorPair,
    LLMUserAggregatorParams,
)
from pipecat.observers.base_observer import BaseObserver, FramePushed
from pipecat.pipeline.task import FrameDirection
from pipecat.processors.frame_processor import FrameProcessor
from pipecat.runner.types import DailyRunnerArguments, RunnerArguments
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.ultravox.llm import OneShotInputParams, UltravoxRealtimeLLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

# Where to save the latest screenshare screenshot (high-res PNG)
SCREENSHARE_OUTPUT_PATH = os.getenv(
    "SCREENSHARE_OUTPUT_PATH",
    os.path.join(os.path.dirname(__file__), "screenshare_latest.png"),
)

# How often to save a screenshare snapshot (seconds)
SCREENSHARE_SAMPLE_INTERVAL_SECS = float(
    os.getenv("SCREENSHARE_SAMPLE_INTERVAL_SECS", "10")
)

# Prediction API (entry.py Flask)
FEEDBACK_PORT = int(os.getenv("FEEDBACK_PORT", "3001"))
PREDICT_URL = os.environ.get("PREDICT_URL", f"http://127.0.0.1:{FEEDBACK_PORT}/predict")
QWEN_STEP_URL = os.environ.get("QWEN_STEP_URL", f"http://127.0.0.1:{FEEDBACK_PORT}/qwen_step")
THREE_STEP_QWEN_URL = os.environ.get("THREE_STEP_QWEN_URL", f"http://127.0.0.1:{FEEDBACK_PORT}/three_step_qwen")
DRAW_POINT_URL = os.environ.get("DRAW_POINT_URL", f"http://127.0.0.1:{FEEDBACK_PORT}/draw_point")

# 3-step IAM: hardcoded coords per step (must match backend)
STEP_COORDINATES = [(630, 700), (890, 560), (800, 330)]
# Delay (seconds) before drawing the line after Qwen response + "Let me draw a line..." (real-call pacing)
THREE_STEP_DRAW_DELAY_SECS = float(os.environ.get("THREE_STEP_DRAW_DELAY_SECS", "3"))

# GPT classification: user's OpenAI key (OPENAI_API_KEY or GPT_KEY)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY") or os.environ.get("GPT_KEY")

# One action flow at a time; pending (step_text, button_label) for tool handlers
action_in_progress = False
pending_instruction: tuple[str, str] | None = None  # (step_text, button_label)

# 3-step IAM flow: we analyze one step at a time (ss1 -> talk -> draw -> permission, then ss2, then ss3)
three_step_data: list[tuple[str, str] | None] = [None, None, None]  # slot per step, filled when we run that step
three_step_active = False
three_step_transcript = ""  # original user message, used when fetching step 2 and 3
three_step_next = 0  # 0 = not waiting; 2 = do step 2 when user confirms; 3 = do step 3 when user confirms
three_step_allowed_draw = 0  # 1, 2, or 3 = only that step's draw is accepted; 0 = none

# Mutable ref so draw_step handler can queue next instruction after each draw
task_ref: list = [None]  # [PipelineTask]; set task_ref[0] = task in run_bot


def _is_confirmation(transcript: str) -> bool:
    """True if user is saying yes/ready/next to move to the next step."""
    t = (transcript or "").strip().lower().rstrip(".!?")
    if not t or len(t) > 40:
        return False
    confirm_words = ("yes", "yeah", "yep", "ready", "next", "go", "sure", "ok", "okay", "continue", "do it", "let's go")
    if t in confirm_words:
        return True
    # Short phrases like "yes please", "ready when you are"
    first = t.split()[0] if t.split() else ""
    return first in confirm_words or any(t.startswith(w) for w in confirm_words)


def _classify_action_sync(transcript: str) -> str:
    """Classify user transcript as 'three_step_iam' | 'action' | 'not'. Run in executor."""
    t = (transcript or "").strip()
    if not t:
        return "not"
    t_lower = t.lower()
    # 3-step IAM: edit my role, add someone, IAM (how to add / edit)
    if ("edit" in t_lower and "role" in t_lower) or "add someone" in t_lower:
        return "three_step_iam"
    if "iam" in t_lower and ("add" in t_lower or "how" in t_lower or "edit" in t_lower):
        return "three_step_iam"
    if not OPENAI_API_KEY:
        return "not"
    try:
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json",
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [
                    {
                        "role": "user",
                        "content": "If the user is asking how to edit their role, add someone to IAM, or similar multi-step IAM task, reply: three_step_iam. Else if they want a single screen action (click a button, find something, open a menu): action. Else: not. Reply only one of: three_step_iam, action, not",
                    },
                    {"role": "user", "content": t},
                ],
                "max_tokens": 15,
            },
            timeout=10,
        )
        if not r.ok:
            logger.warning(f"GPT classify failed {r.status_code}")
            return "not"
        text = (r.json().get("choices") or [{}])[0].get("message", {}).get("content") or ""
        text = text.strip().lower()
        if "three_step_iam" in text:
            return "three_step_iam"
        if "action" in text:
            return "action"
        return "not"
    except Exception as e:
        logger.warning(f"GPT classify: {e}")
        return "not"


def _qwen_step_sync(transcript: str, image_path: str) -> tuple[str, str] | None:
    """Call /qwen_step; return (step_text, button_label) or None. Run in executor."""
    try:
        payload = {
            "prompt": f"The user wants to: {transcript}. What is the first button or link they should click on this screen?",
        }
        if image_path:
            payload["image_path"] = image_path
        r = requests.post(QWEN_STEP_URL, json=payload, timeout=60)
        if not r.ok:
            logger.warning(f"Qwen step failed {r.status_code}")
            return None
        data = r.json()
        return (data.get("step_text", ""), data.get("button_label", ""))
    except Exception as e:
        logger.warning(f"Qwen step: {e}")
        return None


def _draw_on_screen_sync(button_label: str) -> bool:
    """POST /predict with button_label only (simulated coords + draw). Run in executor."""
    try:
        r = requests.post(PREDICT_URL, json={"button_label": button_label}, timeout=60)
        return r.ok
    except Exception as e:
        logger.warning(f"Draw on screen: {e}")
        return False


def _three_step_qwen_one_sync(transcript: str, step_number: int) -> tuple[str, str] | None:
    """Call /three_step_qwen for one step (analyze that step's screenshot); return (step_text, button_label) or None. Run in executor."""
    try:
        r = requests.post(
            THREE_STEP_QWEN_URL,
            json={
                "user_transcript": transcript or "they want to edit their role or add someone to IAM",
                "step_number": step_number,
            },
            timeout=60,
        )
        if not r.ok:
            logger.warning(f"Three-step Qwen step {step_number} failed {r.status_code}")
            return None
        data = r.json()
        return (data.get("step_text", ""), data.get("button_label", ""))
    except Exception as e:
        logger.warning(f"Three-step Qwen step {step_number}: {e}")
        return None


class ActionFlowProcessor(FrameProcessor):
    """Classify transcript; if three_step_iam run 3-step flow, else if action run single-step Qwen + draw."""

    def __init__(self, screenshot_path: str = SCREENSHARE_OUTPUT_PATH, **kwargs):
        super().__init__(name="ActionFlow", **kwargs)
        self._screenshot_path = screenshot_path

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        global action_in_progress, pending_instruction, three_step_data, three_step_active, three_step_next, three_step_transcript, three_step_allowed_draw
        await super().process_frame(frame, direction)

        if not isinstance(frame, TranscriptionFrame) or not (frame.text and frame.text.strip()):
            await self.push_frame(frame, direction)
            return

        transcript = frame.text.strip()
        loop = asyncio.get_event_loop()

        # 3-step flow: user said "yes" / "ready" — step 2/3 use hardcoded text (no Qwen)
        if three_step_active and three_step_next in (2, 3) and _is_confirmation(transcript):
            next_step = three_step_next
            three_step_next = 0
            action_in_progress = True
            await self.push_frame(frame, direction)
            # Step 2 and 3: hardcoded instructions (no Qwen call)
            if next_step == 2:
                step_text = "Now you will have to click that small edit button on the right of your name."
                button_label = "edit button"
            else:
                step_text = "Now go to that dropdown and edit your role."
                button_label = "dropdown"
            three_step_data[next_step - 1] = (step_text, button_label)
            three_step_allowed_draw = next_step
            instruction = (
                f"Say: Step {next_step}. Then call get_step_instruction({next_step}) and say exactly what it returns. "
                "Then say: Let me draw a line to show you where. Then call draw_step({}) only.".format(next_step)
            )
            await self.push_frame(InputTextRawFrame(instruction), direction)
            await self.push_frame(LLMRunFrame(), direction)
            return

        classification = await loop.run_in_executor(None, _classify_action_sync, transcript)
        if classification == "not":
            await self.push_frame(frame, direction)
            return

        if action_in_progress:
            await self.push_frame(frame, direction)
            return

        action_in_progress = True
        await self.push_frame(frame, direction)

        if classification == "three_step_iam":
            # Analyze ss1 first (Qwen), then talk, draw, ask permission; step 2/3 analyzed when user says yes
            result = await loop.run_in_executor(None, _three_step_qwen_one_sync, transcript, 1)
            if not result:
                action_in_progress = False
                logger.warning("Three-step Qwen step 1 fetch failed")
                return
            three_step_data = [result, None, None]
            three_step_transcript = transcript
            three_step_active = True
            three_step_allowed_draw = 1
            instruction = (
                "First say: This is a 3-step process. I'll walk you through one step at a time. "
                "For step 1, call get_step_instruction(1) and say exactly what it returns. "
                "Then say: Let me draw a line to show you where. Then call draw_step(1) only."
            )
        else:
            # single-step action
            result = await loop.run_in_executor(
                None,
                _qwen_step_sync,
                transcript,
                self._screenshot_path,
            )
            if not result:
                action_in_progress = False
                return
            step_text, button_label = result
            pending_instruction = (step_text, button_label)
            instruction = (
                "The user asked for a screen action. Call get_click_instruction with their goal, "
                "then say exactly what it returns, then say 'Let me draw a line for you of where it is.', "
                "then call draw_on_screen()."
            )
        await self.push_frame(InputTextRawFrame(instruction), direction)
        await self.push_frame(LLMRunFrame(), direction)


class ScreenshareScreenshotProcessor(FrameProcessor):
    """Saves screenshared video frames as high-res PNGs every N seconds. Passes all frames through."""

    def __init__(
        self,
        output_path: str = SCREENSHARE_OUTPUT_PATH,
        interval_secs: float = SCREENSHARE_SAMPLE_INTERVAL_SECS,
        **kwargs,
    ):
        super().__init__(name="ScreenshareScreenshot", **kwargs)
        self._output_path = output_path
        self._interval_secs = interval_secs
        self._last_save_time: float = 0.0

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        if (
            isinstance(frame, UserImageRawFrame)
            and getattr(frame, "transport_source", None) == "screenVideo"
        ):
            now = time.monotonic()
            if now - self._last_save_time >= self._interval_secs:
                try:
                    w, h = frame.size
                    fmt = frame.format or "RGB"
                    img = Image.frombytes(fmt, (w, h), frame.image)
                    img.save(self._output_path)
                    self._last_save_time = now
                    logger.debug(
                        f"Saved screenshare frame to {self._output_path} ({w}x{h})"
                    )
                except Exception as e:
                    logger.warning(f"Failed to save screenshare frame: {e}")
        await self.push_frame(frame, direction)


class ErrorLogObserver(BaseObserver):
    """Log full exception when an ErrorFrame is pushed (e.g. Ultravox connection failure)."""

    async def on_push_frame(self, data: FramePushed):
        if isinstance(data.frame, ErrorFrame) and data.frame.exception:
            logger.exception(
                f"Pipeline error (underlying exception): {data.frame.exception}"
            )


# Tool schemas for Ultravox (action flow: get step text, then draw)
GET_CLICK_INSTRUCTION_SCHEMA = FunctionSchema(
    name="get_click_instruction",
    description="Get the instruction for what the user should click on screen (e.g. 'Click the Grant Access button'). Call this when the user asked for a single screen action.",
    properties={
        "goal": {
            "type": "string",
            "description": "The user's goal (what they want to do on screen).",
        },
    },
    required=["goal"],
)
DRAW_ON_SCREEN_SCHEMA = FunctionSchema(
    name="draw_on_screen",
    description="Draw a line on the screen showing where to click. Call this after saying the click instruction and 'Let me draw a line for you of where it is.'",
    properties={},
    required=[],
)
GET_STEP_INSTRUCTION_SCHEMA = FunctionSchema(
    name="get_step_instruction",
    description="Get the pre-fetched instruction for step N of the 3-step IAM flow (edit role / add someone). Say exactly what it returns. Use step_number 1, 2, or 3.",
    properties={
        "step_number": {
            "type": "integer",
            "description": "Step number: 1, 2, or 3.",
        },
    },
    required=["step_number"],
)
DRAW_STEP_SCHEMA = FunctionSchema(
    name="draw_step",
    description="Draw a line at the position for step N of the 3-step IAM flow. Call after saying the step instruction and 'Let me draw a line to show you where.' Use step_number 1, 2, or 3.",
    properties={
        "step_number": {
            "type": "integer",
            "description": "Step number: 1, 2, or 3.",
        },
    },
    required=["step_number"],
)
ACTION_FLOW_TOOLS = ToolsSchema(
    standard_tools=[
        GET_CLICK_INSTRUCTION_SCHEMA,
        DRAW_ON_SCREEN_SCHEMA,
        GET_STEP_INSTRUCTION_SCHEMA,
        DRAW_STEP_SCHEMA,
    ]
)

SYSTEM_PROMPT = """You are a friendly AI assistant. Respond naturally and keep your answers conversational.

When the user wants to do something on the screen (e.g. click a button, find something, add a member):
1. Call get_click_instruction with their goal.
2. Say exactly what the tool returns.
3. Say "Let me draw a line for you of where it is."
4. Call draw_on_screen().

When you are in the 3-step IAM flow (edit role / add someone), we walk through one step at a time. Say exactly what get_step_instruction returns; then say "Let me draw a line to show you where"; then call draw_step. After each step we ask if the user is ready for the next step; when they say yes we continue.

For non-action turns, respond normally. Start by greeting the user: "What can I help you with today?"
"""


async def _get_click_instruction_handler(params: FunctionCallParams) -> None:
    """Return stored step_text for the current action flow."""
    global pending_instruction
    step_text = (pending_instruction or ("", ""))[0]
    await params.result_callback(step_text or "Click the indicated element.")


async def _draw_on_screen_handler(params: FunctionCallParams) -> None:
    """No actual draw; clear action_in_progress. Bot still says 'Let me draw a line...'."""
    global action_in_progress, pending_instruction
    action_in_progress = False
    pending_instruction = None
    await params.result_callback("Done.")


def _get_step_number_from_params(params: FunctionCallParams) -> int:
    """Extract step_number (1-3) from function call arguments."""
    args = getattr(params, "arguments", None) or getattr(params, "args", None) or {}
    if isinstance(args, dict):
        n = args.get("step_number", 1)
    else:
        n = 1
    try:
        n = int(n)
    except (TypeError, ValueError):
        n = 1
    return max(1, min(3, n))


async def _get_step_instruction_handler(params: FunctionCallParams) -> None:
    """Return step_text for the given step (filled when we analyzed that step's screenshot)."""
    global three_step_data
    step_number = _get_step_number_from_params(params)
    if not three_step_data or step_number < 1 or step_number > len(three_step_data):
        await params.result_callback("Click the indicated element.")
        return
    slot = three_step_data[step_number - 1]
    if not slot:
        await params.result_callback("Click the edit icon on the far right of your name.")
        return
    step_text, _ = slot
    await params.result_callback(step_text or "Click the dropdown menu it currently says Owner. Edit the role to what you desire.")


async def _draw_step_handler(params: FunctionCallParams) -> None:
    """Run step N: delay, then ask 'ready for next?' (no draw call by default)."""
    global three_step_data, three_step_active, three_step_next, three_step_allowed_draw, action_in_progress
    step_number = _get_step_number_from_params(params)

    # Only one draw per step: block draw_step(2)/draw_step(3) when we're on step 1, etc.
    if three_step_active and (three_step_allowed_draw == 0 or step_number != three_step_allowed_draw):
        await params.result_callback("Do not draw yet; wait for the current step.")
        return

    # Delay so pacing matches "Let me draw a line..." (no actual draw call)
    await asyncio.sleep(THREE_STEP_DRAW_DELAY_SECS)
    await params.result_callback("Done.")

    # No further draws until we inject the next step
    three_step_allowed_draw = 0

    task = task_ref[0] if task_ref else None
    if not task:
        return

    if step_number < 3:
        # One step at a time: ask for confirmation before next step
        three_step_next = step_number + 1
        action_in_progress = False
        await task.queue_frames([
            InputTextRawFrame("Say: Are we ready to move on? Say yes when you're ready."),
            LLMRunFrame(),
        ])
    else:
        three_step_active = False
        three_step_data = [None, None, None]
        three_step_transcript = ""
        three_step_next = 0
        three_step_allowed_draw = 0
        action_in_progress = False
        await task.queue_frames([
            InputTextRawFrame("Say: You're all set. You've completed all the steps to edit your role."),
            LLMRunFrame(),
        ])


async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    system_prompt = os.getenv("ULTRAVOX_SYSTEM_PROMPT") or SYSTEM_PROMPT
    llm = UltravoxRealtimeLLMService(
        params=OneShotInputParams(
            api_key=os.getenv("ULTRAVOX_API_KEY"),
            system_prompt=system_prompt,
            temperature=0.3,
            max_duration=datetime.timedelta(minutes=3),
        ),
        one_shot_selected_tools=ACTION_FLOW_TOOLS,
    )
    llm.register_function("get_click_instruction", _get_click_instruction_handler, cancel_on_interruption=True)
    llm.register_function("draw_on_screen", _draw_on_screen_handler, cancel_on_interruption=False)
    llm.register_function("get_step_instruction", _get_step_instruction_handler, cancel_on_interruption=True)
    llm.register_function("draw_step", _draw_step_handler, cancel_on_interruption=False)

    messages = [
        {"role": "system", "content": system_prompt},
    ]
    context = LLMContext(messages)
    user_aggregator, assistant_aggregator = LLMContextAggregatorPair(
        context,
        user_params=LLMUserAggregatorParams(
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        ),
    )

    screenshot_processor = ScreenshareScreenshotProcessor()
    action_flow_processor = ActionFlowProcessor(screenshot_path=SCREENSHARE_OUTPUT_PATH)

    pipeline = Pipeline(
        [
            transport.input(),
            screenshot_processor,
            user_aggregator,
            action_flow_processor,
            llm,
            transport.output(),
            assistant_aggregator,
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[ErrorLogObserver()],
    )
    task_ref[0] = task

    # Workaround: RTVI error messages from Ultravox/client often lack "id"; Pipecat RTVIMessage requires it (GH#1484).
    rtvi = task.rtvi
    _orig_handle = rtvi._handle_transport_message
    async def _patched_handle(frame):
        msg = getattr(frame, "message", None)
        if isinstance(msg, dict) and msg.get("label") == "rtvi-ai" and "id" not in msg:
            msg["id"] = str(uuid.uuid4())
        await _orig_handle(frame)
    rtvi._handle_transport_message = _patched_handle

    @task.rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        # Start capturing screen share from remote participants (Daily only)
        if hasattr(transport, "participant_id") and hasattr(transport, "participants"):
            try:
                my_id = transport.participant_id
                participants = transport.participants() or {}
                input_transport = transport.input()
                if hasattr(input_transport, "capture_participant_video"):
                    for pid, info in participants.items():
                        # Daily uses "local" as a key for the local participant; it's not a valid participant ID for capture
                        if pid != my_id and pid != "local":
                            await input_transport.capture_participant_video(
                                pid,
                                framerate=1,
                                video_source="screenVideo",
                                color_format="RGB",
                            )
                            logger.info(f"Capturing screen share from participant {pid}")
            except Exception as e:
                logger.warning(f"Could not start screen capture: {e}")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        global three_step_active, three_step_data, three_step_transcript, three_step_next, three_step_allowed_draw, action_in_progress, pending_instruction
        logger.info("Client disconnected")
        three_step_active = False
        three_step_allowed_draw = 0
        three_step_data = [None, None, None]
        three_step_transcript = ""
        three_step_next = 0
        action_in_progress = False
        pending_instruction = None
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point. Daily only — one bot allowed (no playground/SmallWebRTC)."""
    if not isinstance(runner_args, DailyRunnerArguments):
        logger.warning(
            "Only Daily transport is allowed. Run with: uv run bot.py -t daily. "
            "Ignoring non-Daily connection."
        )
        return

    transport = DailyTransport(
        runner_args.room_url,
        runner_args.token,
        "Aetheris V.O",
        params=DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            video_in_enabled=True,
        ),
    )
    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()
